{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Twitter_Dataset.csv', encoding= 'latin1')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import emoji\n",
    "\n",
    "# Function to calculate the word count\n",
    "def word_count(text):\n",
    "    words = text.split()\n",
    "    return len(words)\n",
    "\n",
    "# Function to calculate the character count\n",
    "def character_count(text):\n",
    "    return len(text)\n",
    "\n",
    "# Function to calculate the average word length\n",
    "def average_word_length(text):\n",
    "    words = text.split()\n",
    "    word_lengths = [len(word) for word in words]\n",
    "    return sum(word_lengths) / len(words) if len(words) > 0 else 0\n",
    "\n",
    "# Function to calculate the sentence count\n",
    "def sentence_count(text):\n",
    "    sentences = re.split(r'[.!?]+', text)\n",
    "    return len(sentences)\n",
    "\n",
    "# Function to calculate the hashtag count\n",
    "def hashtag_count(text):\n",
    "    hashtags = re.findall(r\"#\\w+\", text)\n",
    "    return len(hashtags)\n",
    "\n",
    "# Function to calculate the mention count\n",
    "def mention_count(text):\n",
    "    mentions = re.findall(r\"@\\w+\", text)\n",
    "    return len(mentions)\n",
    "\n",
    "# Function to calculate the emoji count\n",
    "def emoji_count(text):\n",
    "    text = emoji.demojize(text)\n",
    "    emojis = re.findall(r\":[^:\\s]+:\", text)\n",
    "    return len(emojis)\n",
    "\n",
    "# Function to calculate the URL count\n",
    "def url_count(text):\n",
    "    urls = re.findall(r\"http\\S+|www\\S+\", text)\n",
    "    return len(urls)\n",
    "\n",
    "# Calculate the statistical features for all tweets in the dataset\n",
    "sum_word_count = 0\n",
    "sum_character_count = 0\n",
    "sum_avg_word_length = 0\n",
    "sum_sentence_count = 0\n",
    "sum_hashtag_count = 0\n",
    "sum_mention_count = 0\n",
    "sum_emoji_count = 0\n",
    "sum_url_count = 0\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    tweet = row['tweet']\n",
    "    sum_word_count += word_count(tweet)\n",
    "    sum_character_count += character_count(tweet)\n",
    "    sum_avg_word_length += average_word_length(tweet)\n",
    "    sum_sentence_count += sentence_count(tweet)\n",
    "    sum_hashtag_count += hashtag_count(tweet)\n",
    "    sum_mention_count += mention_count(tweet)\n",
    "    sum_emoji_count += emoji_count(tweet)\n",
    "    sum_url_count += url_count(tweet)\n",
    "\n",
    "# Print the statistical features for the whole dataset\n",
    "print(\"Statistical Features Before Cleaning the Dataset\")\n",
    "print(\"Total Word Count:\", sum_word_count)\n",
    "print(\"Total Character Count:\", sum_character_count)\n",
    "print(\"Average Word Length:\", sum_avg_word_length / len(df))\n",
    "print(\"Total Sentence Count:\", sum_sentence_count)\n",
    "print(\"Total Hashtag Count:\", sum_hashtag_count)\n",
    "print(\"Total Mention Count:\", sum_mention_count)\n",
    "print(\"Total Emoji Count:\", sum_emoji_count)\n",
    "print(\"Total URL Count:\", sum_url_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[\"Class\"].value_counts())\n",
    "print(\"total = \",df[\"Class\"].value_counts().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_class = ['neu', 'neg', 'pos']\n",
    "plt.figure(figsize=(10,4))\n",
    "df[\"Class\"].value_counts().plot(kind='bar');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_row(index):\n",
    "    row = df[df.index == index][['tweet', 'Class']].values[0]\n",
    "    if len(row) > 0:\n",
    "        print(row[0])\n",
    "        print('Class =', row[1])\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#before cleaning\n",
    "for i in range(0,10):\n",
    "   print_row (i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replaceBySpace = re.compile('[/(){}\\[\\]\\|@,.;#]')\n",
    "badSymbols = re.compile(r\"http\\S+|www\\S+|[^0-9a-z #+_?]+\")\n",
    "\n",
    "def clean(text):\n",
    "    \n",
    "    text = text.lower() #lower text\n",
    "    text = replaceBySpace.sub(' ',text) # replace with space which symbols are in replaceBySpace\n",
    "    text = badSymbols.sub('', text) # delete symbols which are in badSymbols\n",
    "    text = ' '.join(word for word in text.split() if word not in stop_words)\n",
    "    return text    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = list(string.ascii_lowercase)\n",
    "stop_words = []\n",
    "for i in range(len(alpha)):\n",
    "    stop_words.append(alpha[i])\n",
    "    for j in range(len(alpha)):\n",
    "        stop_words.append(alpha[i]+alpha[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"tweet\"]= df[\"tweet\"].apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"tweet\"] = df[\"tweet\"].apply(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "stop_words = list(set(stopwords.words('english')))\n",
    "lemma = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Class = df['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower = [clean(i).lower() for i in df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list1 = []\n",
    "for i in lower:\n",
    "    words = word_tokenize(i)\n",
    "    list1.append(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list2 = []\n",
    "for i in list1:\n",
    "    l = []\n",
    "    for j in i:\n",
    "        if j not in stop_words:\n",
    "            lem = lemma.lemmatize(j)\n",
    "            l.append(lem)\n",
    "    y = ' '.join(l)\n",
    "    list2.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_words = df['tweet'].apply(lambda x: len(x.split(' '))).sum()\n",
    "print(total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#before cleaning\n",
    "for i in range(0,10):\n",
    "   print_row (i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import emoji\n",
    "\n",
    "# Function to calculate the word count\n",
    "def word_count(text):\n",
    "    words = text.split()\n",
    "    return len(words)\n",
    "\n",
    "# Function to calculate the character count\n",
    "def character_count(text):\n",
    "    return len(text)\n",
    "\n",
    "# Function to calculate the average word length\n",
    "def average_word_length(text):\n",
    "    words = text.split()\n",
    "    word_lengths = [len(word) for word in words]\n",
    "    return sum(word_lengths) / len(words) if len(words) > 0 else 0\n",
    "\n",
    "# Function to calculate the sentence count\n",
    "def sentence_count(text):\n",
    "    sentences = re.split(r'[.!?]+', text)\n",
    "    return len(sentences)\n",
    "\n",
    "# Function to calculate the hashtag count\n",
    "def hashtag_count(text):\n",
    "    hashtags = re.findall(r\"#\\w+\", text)\n",
    "    return len(hashtags)\n",
    "\n",
    "# Function to calculate the mention count\n",
    "def mention_count(text):\n",
    "    mentions = re.findall(r\"@\\w+\", text)\n",
    "    return len(mentions)\n",
    "\n",
    "# Function to calculate the emoji count\n",
    "def emoji_count(text):\n",
    "    text = emoji.demojize(text)\n",
    "    emojis = re.findall(r\":[^:\\s]+:\", text)\n",
    "    return len(emojis)\n",
    "\n",
    "# Function to calculate the URL count\n",
    "def url_count(text):\n",
    "    urls = re.findall(r\"http\\S+|www\\S+\", text)\n",
    "    return len(urls)\n",
    "\n",
    "# Calculate the statistical features for all tweets in the dataset\n",
    "sum_word_count = 0\n",
    "sum_character_count = 0\n",
    "sum_avg_word_length = 0\n",
    "sum_sentence_count = 0\n",
    "sum_hashtag_count = 0\n",
    "sum_mention_count = 0\n",
    "sum_emoji_count = 0\n",
    "sum_url_count = 0\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    tweet = row['tweet']\n",
    "    sum_word_count += word_count(tweet)\n",
    "    sum_character_count += character_count(tweet)\n",
    "    sum_avg_word_length += average_word_length(tweet)\n",
    "    sum_sentence_count += sentence_count(tweet)\n",
    "    sum_hashtag_count += hashtag_count(tweet)\n",
    "    sum_mention_count += mention_count(tweet)\n",
    "    sum_emoji_count += emoji_count(tweet)\n",
    "    sum_url_count += url_count(tweet)\n",
    "\n",
    "# Print the statistical features for the whole dataset\n",
    "print(\"Statistical Features After Cleaning the Dataset\")\n",
    "print(\"Total Word Count:\", sum_word_count)\n",
    "print(\"Total Character Count:\", sum_character_count)\n",
    "print(\"Average Word Length:\", sum_avg_word_length / len(df))\n",
    "print(\"Total Sentence Count:\", sum_sentence_count)\n",
    "print(\"Total Hashtag Count:\", sum_hashtag_count)\n",
    "print(\"Total Mention Count:\", sum_mention_count)\n",
    "print(\"Total Emoji Count:\", sum_emoji_count)\n",
    "print(\"Total URL Count:\", sum_url_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[\"tweet\"]\n",
    "y = df[\"Class\"]\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    \n",
    "model_list = [ Pipeline([\n",
    "    ('vect', TfidfVectorizer()),\n",
    "    ('clf', MLPClassifier(hidden_layer_sizes=(47), max_iter=100, random_state=42)),\n",
    "]),\n",
    "             Pipeline([\n",
    "    ('vect', TfidfVectorizer()),\n",
    "    ('clf', MLPClassifier(hidden_layer_sizes=(47), max_iter=100, random_state=42)),\n",
    "]),\n",
    "              Pipeline([\n",
    "    ('vect', TfidfVectorizer()),\n",
    "    ('clf', MLPClassifier(hidden_layer_sizes=(47), max_iter=100, random_state=42)),\n",
    "]),\n",
    "              Pipeline([\n",
    "    ('vect', TfidfVectorizer()),\n",
    "    ('clf', MLPClassifier(hidden_layer_sizes=(47), max_iter=100, random_state=42)),\n",
    "]),\n",
    "             Pipeline([\n",
    "    ('vect', TfidfVectorizer()),\n",
    "    ('clf', MLPClassifier(hidden_layer_sizes=(47), max_iter=100, random_state=42)),\n",
    "]),\n",
    "              Pipeline([\n",
    "    ('vect', TfidfVectorizer()),\n",
    "    ('clf', MLPClassifier(hidden_layer_sizes=(47), max_iter=100, random_state=42)),\n",
    "]),\n",
    "              Pipeline([\n",
    "    ('vect', TfidfVectorizer()),\n",
    "    ('clf', MLPClassifier(hidden_layer_sizes=(47), max_iter=100, random_state=42)),\n",
    "]),\n",
    "              Pipeline([\n",
    "    ('vect', TfidfVectorizer()),\n",
    "    ('clf', MLPClassifier(hidden_layer_sizes=(47), max_iter=100, random_state=42)),\n",
    "]),\n",
    "              Pipeline([\n",
    "    ('vect', TfidfVectorizer()),\n",
    "    ('clf', MLPClassifier(hidden_layer_sizes=(45), max_iter=100, random_state=42)),\n",
    "]),\n",
    "              Pipeline([\n",
    "    ('vect', TfidfVectorizer()),\n",
    "    ('clf', MLPClassifier(hidden_layer_sizes=(45), max_iter=100, random_state=42)),\n",
    "])\n",
    "             ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_values(pc, fmt=\"%.2f\", **kw):\n",
    "\n",
    "    pc.update_scalarmappable()\n",
    "    ax = pc.axes# FOR LATEST MATPLOTLIB\n",
    "    #Use zip BELOW IN PYTHON 3\n",
    "    for p, color, value in zip(pc.get_paths(), pc.get_facecolors(), pc.get_array()):\n",
    "        x, y = p.vertices[:-2, :].mean(0)\n",
    "        if np.all(color[:3] > 0.5):\n",
    "            color = (0.0, 0.0, 0.0)\n",
    "        else:\n",
    "            color = (1.0, 1.0, 1.0)\n",
    "        ax.text(x, y, fmt % value, ha=\"center\", va=\"center\", color=color, **kw)\n",
    "\n",
    "\n",
    "def cm2inch(*tupl):\n",
    "\n",
    "    inch = 2.54\n",
    "    if type(tupl[0]) == tuple:\n",
    "        return tuple(i/inch for i in tupl[0])\n",
    "    else:\n",
    "        return tuple(i/inch for i in tupl)\n",
    "\n",
    "\n",
    "def heatmap(AUC, title, xlabel, ylabel, xticklabels, yticklabels, figure_width=40, figure_height=20, correct_orientation=False, cmap='RdBu'):\n",
    "\n",
    "\n",
    "    # Plot it out\n",
    "    fig, ax = plt.subplots()    \n",
    "    c = ax.pcolor(AUC, edgecolors='k', linestyle= 'dashed', linewidths=0.2, cmap='RdYlGn', vmin=0.90, vmax=1.0)\n",
    "    #c = ax.pcolor(AUC, edgecolors='k', linestyle= 'dashed', linewidths=0.2, cmap=cmap)\n",
    "\n",
    "    # put the major ticks at the middle of each cell\n",
    "    ax.set_yticks(np.arange(AUC.shape[0]) + 0.5, minor=False)\n",
    "    ax.set_xticks(np.arange(AUC.shape[1]) + 0.5, minor=False)\n",
    "\n",
    "    # set tick labels\n",
    "    #ax.set_xticklabels(np.arange(1,AUC.shape[1]+1), minor=False)\n",
    "    ax.set_xticklabels(xticklabels, minor=False)\n",
    "    ax.set_yticklabels(yticklabels, minor=False)\n",
    "\n",
    "    # set title and x/y labels\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)      \n",
    "\n",
    "    # Remove last blank column\n",
    "    plt.xlim( (0, AUC.shape[1]) )\n",
    "\n",
    "    # Turn off all the ticks\n",
    "    ax = plt.gca()    \n",
    "    for t in ax.xaxis.get_major_ticks():\n",
    "        t.tick1line.set_visible = False\n",
    "        t.tick2line.set_visible = False\n",
    "    for t in ax.yaxis.get_major_ticks():\n",
    "        t.tick1line.set_visible = False\n",
    "        t.tick2line.set_visible = False\n",
    "\n",
    "    # Add color bar\n",
    "    plt.colorbar(c)\n",
    "\n",
    "    # Add text in each cell \n",
    "    show_values(c)\n",
    "\n",
    "    # Proper orientation (origin at the top left instead of bottom left)\n",
    "    if correct_orientation:\n",
    "        ax.invert_yaxis()\n",
    "        ax.xaxis.tick_top()       \n",
    "\n",
    "    # resize \n",
    "    fig = plt.gcf()\n",
    "    fig.set_size_inches(cm2inch(35, 20))\n",
    "    #fig.set_size_inches(cm2inch(40*4, 20*4))\n",
    "    #fig.set_size_inches(cm2inch(figure_width, figure_height))\n",
    "\n",
    "\n",
    "\n",
    "def plot_classification_report(classification_report, title='Classification report ', cmap='RdBu'):\n",
    "\n",
    "    lines = classification_report.split('\\n')\n",
    "\n",
    "    classes = []\n",
    "    plotMat = []\n",
    "    support = []\n",
    "    class_names = []\n",
    "    for line in lines[2 : 6]:\n",
    "        t = line.strip().split()\n",
    "        if len(t) < 2: continue\n",
    "        classes.append(t[0])\n",
    "        v = [float(x) for x in t[1: len(t) - 1]]\n",
    "        support.append(int(t[-1]))\n",
    "        class_names.append(t[0])\n",
    "        #print(v)\n",
    "        plotMat.append(v)\n",
    "\n",
    "    #print('plotMat: {0}'.format(plotMat))\n",
    "    #print('support: {0}'.format(support))\n",
    "\n",
    "    xlabel = 'Metrics'\n",
    "    ylabel = 'Classes'\n",
    "    xticklabels = ['Precision', 'Recall', 'F1-score']\n",
    "    yticklabels = ['{0} ({1})'.format(class_names[idx], sup) for idx, sup  in enumerate(support)]\n",
    "    figure_width = 25\n",
    "    figure_height = len(class_names) + 7\n",
    "    correct_orientation = False\n",
    "    heatmap(np.array(plotMat), title, xlabel, ylabel, xticklabels, yticklabels, figure_width, figure_height, correct_orientation, cmap=cmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "mlp_log = Pipeline([\n",
    "    ('vect', TfidfVectorizer()),\n",
    "    ('clf', MLPClassifier(hidden_layer_sizes=(45), max_iter=100, random_state=42)),\n",
    "])\n",
    "mlp_log.fit(X_train, y_train)\n",
    "y_pred = mlp_log.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(\"Test Accuracy of MLP:\", acc)\n",
    "report = classification_report(y_test, y_pred, labels=my_class)\n",
    "print(\"Test Report\")\n",
    "print(report)\n",
    "plot_classification_report(report)\n",
    "\n",
    "# Assuming y_true and y_pred are the true and predicted labels, respectively\n",
    "average_precision = precision_score(y_test, y_pred, average='macro')\n",
    "average_recall = recall_score(y_test, y_pred, average='macro')\n",
    "average_f1_score = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "print(\"Average Precision:\", average_precision)\n",
    "print(\"Average Recall:\", average_recall)\n",
    "print(\"Average F1-score:\", average_f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heatmap_cf(AUC, title, xlabel, ylabel, xticklabels, yticklabels, figure_width=40, figure_height=20, correct_orientation=False, cmap='RdBu', mx = 1000):\n",
    "\n",
    "\n",
    "\n",
    "    # Plot it out\n",
    "    fig, ax = plt.subplots()    \n",
    "    c = ax.pcolor(AUC, edgecolors='k', linestyle= 'dashed', linewidths=0.2, cmap='RdYlGn', vmin=0.0, vmax=mx)\n",
    "    #c = ax.pcolor(AUC, edgecolors='k', linestyle= 'dashed', linewidths=0.2, cmap=cmap)\n",
    "\n",
    "    # put the major ticks at the middle of each cell\n",
    "    ax.set_yticks(np.arange(AUC.shape[0]) + 0.5, minor=False)\n",
    "    ax.set_xticks(np.arange(AUC.shape[1]) + 0.5, minor=False)\n",
    "\n",
    "    # set tick labels\n",
    "    #ax.set_xticklabels(np.arange(1,AUC.shape[1]+1), minor=False)\n",
    "    ax.set_xticklabels(xticklabels, minor=False)\n",
    "    ax.set_yticklabels(yticklabels, minor=False)\n",
    "\n",
    "    # set title and x/y labels\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)      \n",
    "\n",
    "    # Remove last blank column\n",
    "    plt.xlim( (0, AUC.shape[1]) )\n",
    "\n",
    "    # Turn off all the ticks\n",
    "    ax = plt.gca()    \n",
    "    for t in ax.xaxis.get_major_ticks():\n",
    "        t.tick1line.set_visible = False\n",
    "        t.tick2line.set_visible = False\n",
    "    for t in ax.yaxis.get_major_ticks():\n",
    "        t.tick1line.set_visible = False\n",
    "        t.tick2line.set_visible = False\n",
    "\n",
    "    # Add color bar\n",
    "    plt.colorbar(c)\n",
    "\n",
    "    # Add text in each cell \n",
    "    show_values(c)\n",
    "\n",
    "    # Proper orientation (origin at the top left instead of bottom left)\n",
    "    if correct_orientation:\n",
    "        ax.invert_yaxis()\n",
    "        ax.xaxis.tick_top()       \n",
    "\n",
    "    # resize \n",
    "    fig = plt.gcf()\n",
    "    fig.set_size_inches(cm2inch(35, 20))\n",
    "    #fig.set_size_inches(cm2inch(40*4, 20*4))\n",
    "    #fig.set_size_inches(cm2inch(figure_width, figure_height))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cf_matrix = confusion_matrix(y_test, y_pred, labels=my_Class)\n",
    "my_Class_rev = my_Class.reverse\n",
    "heatmap_cf(cf_matrix, title='Confusion Matrix' , xlabel=\"True\", ylabel=\"Predicted\", xticklabels=my_Class, yticklabels=my_Class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "\n",
    "def plot_learning_curve(estimator, title, X, y, axes=None, ylim=None, cv=None,\n",
    "                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "\n",
    "    if axes is None:\n",
    "        _, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
    "\n",
    "    axes[0].set_title(title)\n",
    "    if ylim is not None:\n",
    "        axes[0].set_ylim(*ylim)\n",
    "    axes[0].set_xlabel(\"Training examples\")\n",
    "    axes[0].set_ylabel(\"Score\")\n",
    "\n",
    "    train_sizes, train_scores, test_scores, fit_times, _ = \\\n",
    "        learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs,\n",
    "                       train_sizes=train_sizes,\n",
    "                       return_times=True)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    fit_times_mean = np.mean(fit_times, axis=1)\n",
    "    fit_times_std = np.std(fit_times, axis=1)\n",
    "\n",
    "    # Plot learning curve\n",
    "    axes[0].grid()\n",
    "    axes[0].fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                         train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                         color=\"r\")\n",
    "    axes[0].fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                         test_scores_mean + test_scores_std, alpha=0.1,\n",
    "                         color=\"g\")\n",
    "    axes[0].plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "                 label=\"Training score\")\n",
    "    axes[0].plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "                 label=\"Cross-validation score\")\n",
    "    axes[0].legend(loc=\"best\")\n",
    "\n",
    "    # Plot n_samples vs fit_times\n",
    "    axes[1].grid()\n",
    "    axes[1].plot(train_sizes, fit_times_mean, 'o-')\n",
    "    axes[1].fill_between(train_sizes, fit_times_mean - fit_times_std,\n",
    "                         fit_times_mean + fit_times_std, alpha=0.1)\n",
    "    axes[1].set_xlabel(\"Training examples\")\n",
    "    axes[1].set_ylabel(\"fit_times\")\n",
    "    axes[1].set_title(\"Scalability of the model\")\n",
    "\n",
    "    # Plot fit_time vs score\n",
    "    axes[2].grid()\n",
    "    axes[2].plot(fit_times_mean, test_scores_mean, 'o-')\n",
    "    axes[2].fill_between(fit_times_mean, test_scores_mean - test_scores_std,\n",
    "                         test_scores_mean + test_scores_std, alpha=0.1)\n",
    "    axes[2].set_xlabel(\"fit_times\")\n",
    "    axes[2].set_ylabel(\"Score\")\n",
    "    axes[2].set_title(\"Performance of the model\")\n",
    "\n",
    "    return plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3,figsize=(10, 15))\n",
    "title = \"Learning Curve (Multinomial Logistic Regression)\"\n",
    "cv = ShuffleSplit(n_splits=9, test_size=0.3, random_state=42)\n",
    "\n",
    "estimator = Pipeline([('vect', CountVectorizer()),\n",
    "            ('tfidf', TfidfTransformer()),\n",
    "            ('clf', LogisticRegression(n_jobs=5, C=1e5, solver='lbfgs', multi_class='auto')),\n",
    "           ])\n",
    "plot_learning_curve(estimator, title, X, y, axes=axes[:], ylim=(0.7, 1.01),\n",
    "                    cv=cv, n_jobs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "validation_accuracy = []\n",
    "validation_report = []\n",
    "\n",
    "for k, (train, test) in enumerate(kf.split(X,y)):\n",
    "    xtrain, ytrain = X[train], y[train]\n",
    "    xtest, ytest = X[test], y[test]\n",
    "    \n",
    "    model_list[k].fit(xtrain, ytrain)\n",
    "    pred = model_list[k].predict(xtest)\n",
    "    accuracy = accuracy_score(ytest, pred)\n",
    "    report = classification_report(ytest, pred, labels=my_Class)\n",
    "    validation_accuracy.append(accuracy)\n",
    "    validation_report.append(report)\n",
    "    print(f\"Fold #{k+1}\")\n",
    "    print(f\"Validation Accuracy = {accuracy}\")\n",
    "    print(\"Validation Report:\")\n",
    "    print(report)\n",
    "\n",
    "for i in range(9):\n",
    "    if i < len(validation_report):  # Check if index is within the valid range\n",
    "        plot_classification_report(validation_report[i])\n",
    "    \n",
    "prec_fold = []\n",
    "reca_fold = []\n",
    "f1_fold = []\n",
    "for report in validation_report:\n",
    "    lines = list(report.split('\\n'))\n",
    "    prec, reca, f1 = 0, 0, 0\n",
    "    for i in range(2, 6):\n",
    "        split_result = lines[i].split()\n",
    "        if len(split_result) >= 4:  # Check if split result has at least 4 parts\n",
    "            prec += float(split_result[1])\n",
    "            reca += float(split_result[2])\n",
    "            f1 += float(split_result[3])        \n",
    "    prec_fold.append(prec / 4)\n",
    "    reca_fold.append(reca / 4)\n",
    "    f1_fold.append(f1 / 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_n = pd.DataFrame([['01', '01', '01', '02', '02', '02', '03', '03', '03', '04', '04', '04', '05', '05', '05','06','06','06','07','07','07','08','08','08','09','09','09','10','10','10'],\n",
    "                     ['Precision','Recall','F1-Score','Precision','Recall','F1-Score','Precision','Recall','F1-Score','Precision','Recall','F1-Score', 'Precision','Recall','F1-Score','Precision','Recall','F1-Score','Precision','Recall','F1-Score','Precision','Recall','F1-Score', 'Precision','Recall','F1-Score','Precision','Recall','F1-Score'], \n",
    "                     [prec_fold[i] for i in range(len(prec_fold))] + [reca_fold[i] for i in range(len(reca_fold))] + [f1_fold[i] for i in range(len(f1_fold))]]).T\n",
    "\n",
    "df_n.columns = ['Fold', 'Type', 'Score']\n",
    "df_n.set_index(['Fold', 'Type'], inplace=True)\n",
    "ax.set_ylim(2,10)\n",
    "df_n.unstack().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = df['tweet']\n",
    "labels = df['Class']\n",
    "\n",
    "# Generate the IDF weights for all the words in the tweets\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_weights = vectorizer.fit_transform(tweets)\n",
    "idf = vectorizer.idf_\n",
    "tfidf_dict = dict(zip(vectorizer.get_feature_names(), vectorizer.idf_))\n",
    "\n",
    "# Generate the term frequency for all the words in the tweets\n",
    "tf_vectorizer = CountVectorizer()\n",
    "tf = tf_vectorizer.fit_transform(tweets)\n",
    "word_freq = dict(zip(tf_vectorizer.get_feature_names(), np.asarray(tf.sum(axis=0)).ravel()))\n",
    "\n",
    "# Visualize the IDF weights of the most tweeted words\n",
    "sorted_idf = sorted(tfidf_dict.items(), key=lambda x: x[1], reverse=True)[:20]\n",
    "labels, values = zip(*sorted_idf)\n",
    "plt.figure(figsize=[10, 5])\n",
    "plt.bar(labels, values)\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('IDF weights of most tweeted words')\n",
    "plt.ylabel('IDF weights')\n",
    "plt.show()\n",
    "\n",
    "# Visualize the term frequency of the most tweeted words\n",
    "sorted_freq = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:20]\n",
    "labels, values = zip(*sorted_freq)\n",
    "plt.figure(figsize=[10, 5])\n",
    "plt.bar(labels, values)\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Term frequency of most tweeted words')\n",
    "plt.ylabel('Term frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the sentiments and their corresponding metrics\n",
    "sentiments = ['Neutral', 'Negative', 'Positive']\n",
    "precision = [0.97, 0.97, 0.97]  # Replace with your actual precision values\n",
    "recall = [0.91, 0.92, 0.92]  # Replace with your actual recall values\n",
    "f1_score = [0.88, 0.84, 0.86]  # Replace with your actual F1-score values\n",
    "\n",
    "# Set the position of each sentiment on the x-axis\n",
    "x_pos = np.arange(len(sentiments))\n",
    "\n",
    "# Set the width of the bars\n",
    "bar_width = 0.1\n",
    "\n",
    "# Create the bar graph\n",
    "plt.bar(x_pos, precision, width=bar_width, label='Precision')\n",
    "plt.bar(x_pos + bar_width, recall, width=bar_width, label='Recall')\n",
    "plt.bar(x_pos + (2 * bar_width), f1_score, width=bar_width, label='F1 Score')\n",
    "\n",
    "# Set the labels for the x-axis and y-axis\n",
    "plt.xlabel('Sentiments')\n",
    "plt.ylabel('Scores')\n",
    "\n",
    "# Set the title of the graph\n",
    "plt.title('Metrics Comparison for Different Sentiments')\n",
    "\n",
    "# Set the position of the x-axis ticks and label them with the sentiments\n",
    "plt.xticks(x_pos + bar_width, sentiments)\n",
    "\n",
    "# Add a legend\n",
    "plt.legend()\n",
    "\n",
    "# Show the bar graph\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Prepare your data\n",
    "training_samples = ['50:50', '60:40', '70:30','80:20','90:10']\n",
    "precision_values = [89, 90, 92, 92, 92]\n",
    "recall_values = [87, 89, 91, 90, 90]\n",
    "f1score_values = [88, 90, 91, 91,  91]\n",
    "\n",
    "# Create the bar graph\n",
    "plt.figure(figsize=(7, 5))\n",
    "\n",
    "# Calculate the percentage values\n",
    "precision_percentages = np.array(precision_values) / 100\n",
    "recall_percentages = np.array(recall_values) / 100\n",
    "f1score_percentages = np.array(f1score_values) / 100\n",
    "\n",
    "# Set the x-axis positions\n",
    "x_pos = np.arange(len(training_samples))\n",
    "\n",
    "# Set the bar width\n",
    "bar_width = 0.15\n",
    "\n",
    "# Create the bars for each metric\n",
    "plt.bar(x_pos, precision_percentages, width=bar_width, label='Precision')\n",
    "plt.bar(x_pos + bar_width, recall_percentages, width=bar_width, label='Recall')\n",
    "plt.bar(x_pos + 2 * bar_width, f1score_percentages, width=bar_width, label='F1-score')\n",
    "\n",
    "# Set the labels for the x-axis and y-axis\n",
    "plt.xlabel('Tweet Comparison')\n",
    "plt.ylabel('Performance')\n",
    "plt.xticks(x_pos + bar_width, training_samples)\n",
    "plt.yticks(np.arange(0.1, 1.1, 0.1), ['10%', '20%', '30%', '40%', '50%', '60%', '70%', '80%', '90%', '100%'])\n",
    "\n",
    "# Set the title and legend\n",
    "plt.title('Metrics for Different Training and Testing Samples')\n",
    "plt.legend()\n",
    "\n",
    "# Show the graph\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Model names\n",
    "models = ['Proposed Model', 'Logistic Regression', 'Graph-based DL Model','BERT and SVM','Random Forest','SVM', 'Other ML Models']\n",
    "\n",
    "# Accuracy scores\n",
    "acc_scores = [95.14, 94.7, 90.3, 87.5, 80.9, 85.6, 86.4]\n",
    "\n",
    "# Define colors for each bar\n",
    "colors = ['#ff1493', '#00ced1','#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd']\n",
    "\n",
    "# Set the x-coordinates for the bars and labels\n",
    "x_pos = np.arange(len(models))\n",
    "x_size = 0.9\n",
    "\n",
    "# Create a 3D bar chart\n",
    "fig = plt.figure(figsize=(10, 12))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "for i in range(len(models)):\n",
    "    ax.bar3d(x_pos[i], i, 0, x_size, 0.5, acc_scores[i], color=colors[i], alpha=0.8)\n",
    "    ax.text(x_pos[i]+x_size/2, i, acc_scores[i]+1, str(acc_scores[i]), ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "# Set the title and axis labels\n",
    "ax.set_xlabel('Models')\n",
    "ax.set_zlabel('Accuracy')\n",
    "\n",
    "scale_factor = 1.0\n",
    "\n",
    "# Set the x-axis limits\n",
    "ax.set_xlim([-1, len(models)])\n",
    "ax.set_xticks(x_pos)\n",
    "\n",
    "\n",
    "# Set the y-axis limits\n",
    "ax.set_ylim([-1, len(models)])\n",
    "ax.set_yticks(x_pos)\n",
    "ax.set_yticklabels(models)\n",
    "\n",
    "# Set the z-axis limits\n",
    "ax.set_zlim([0, max(acc_scores)+10])\n",
    "\n",
    "# Customize the legend\n",
    "handles = [plt.Rectangle((0,0),1,1, color=colors[i], alpha=0.8) for i in range(len(models))]\n",
    "labels = models\n",
    "ax.legend(handles, labels, loc='center')\n",
    "\n",
    "# Rotate the chart for better view\n",
    "ax.view_init(elev=7, azim=-30)\n",
    "\n",
    "# Display the plot\n",
    "\n",
    "plt.yticks(rotation = 50)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabel = pd.read_csv('COVdataset unlabeled.csv', encoding='latin1')\n",
    "unlabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabel[\"tweet\"].apply(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = multinomial_log.predict(unlabel[\"tweet\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabel[\"Class\"] = [cls for cls in predicted]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabel.to_csv(\"classified covid.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
